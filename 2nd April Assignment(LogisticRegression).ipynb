{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3348c9c0-b77d-49dc-93af-0ae31a817f63",
   "metadata": {},
   "source": [
    "# Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
    "\n",
    "GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. It's essentially a cross-validation technique. The model as well as the parameters must be entered. It is a technique for hyperparameter optimization in machine learning. It allows us to exhaustively search over a specified range of values for different parameters of an estimator, such as a classifier or a regressor, and find the best combination of parameters that maximizes the performance of the model on a given metric, such as accuracy or f1-score. The performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values. Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters. GridSearchCV is a function that comes in Scikit-learn’s model_selection package.\n",
    "\n",
    "Grid search cv works by creating a grid of all possible combinations of the parameter values that we specify, and then evaluating each combination using cross-validation. Cross-validation is a method of splitting the data into multiple subsets, such as training and testing sets, and then training and testing the model on each subset to measure its generalization ability. Grid search cv returns the best estimator, which is the one that achieved the highest score on the cross-validation, as well as the best parameters, which are the values that were used for that estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c55bf0-756a-474a-a955-25bfc6916f23",
   "metadata": {},
   "source": [
    "# Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?\n",
    "\n",
    "The only difference between both the approaches is in grid search we define the combinations and do training of the model whereas in RandomizedSearchCV the model selects the combinations randomly.\n",
    "\n",
    "In Grid Search, we try every combination of a preset list of values of the hyper-parameters and choose the best combination based on the cross-validation score.\n",
    "\n",
    "Random search tries random combinations of a range of values (we have to define the number iterations). It is good at testing a wide range of values and normally it reaches a very good combination very fast, but the problem that it doesn’t guarantee to give the best parameter combination.\n",
    "\n",
    "On the other hand, Grid search will give the best combination but it can take a lot of time.\n",
    "\n",
    "We might choose grid search cv over randomize search cv when we want to be more precise and reproducible, and when we have prior knowledge about the best parameter values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d142baf0-354e-4abe-91c9-c59f6001d58c",
   "metadata": {},
   "source": [
    "# Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
    "\n",
    "A scenario when ML model already has information of test data in training data, but this information would not be available at the time of prediction, called data leakage. It causes high performance while training set, but perform poorly in deployment or production. Data leakage generally occurs when the training data is overlapped with testing data during the development process of ML models by sharing information between both data sets. Ideally, there should not be any interaction between these data sets (training and test sets). Still, sharing data between tests and training data sets is an accidental scenario that leads to the bad performance of the models.\n",
    "\n",
    "Data leakage is a problem in machine learning because it can lead to invalid or biased models that do not reflect the true underlying problem or data distribution. Data leakage can also result in privacy breaches, ethical issues, or financial losses if the model is used for decision making or deployed in production.\n",
    "\n",
    "Example, suppose we want to build a model to predict whether a patient will be readmitted to the hospital within 30 days based on their medical records. If we include the discharge date or diagnosis as features in our model, we are leaking information from the future that would not be known at the time of admission. This can make our model learn spurious patterns that do not reflect the true risk factors for readmission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfcafe1-74d5-473b-969f-af8a744042ed",
   "metadata": {},
   "source": [
    "# Q4. How can you prevent data leakage when building a machine learning model?\n",
    "\n",
    "some best practices for avoiding data leakage in machine learning:\n",
    "\n",
    "1. Understand the problem domain: It is important to understand the problem domain and the potential sources of data leakage. \n",
    "2. Use proper data splitting: Proper data splitting is essential for preventing data leakage. Use techniques such as stratified sampling to ensure that the distribution of the target variable is consistent across the training, validation, and test sets.\n",
    "3. Use cross-validation: Cross-validation is a powerful technique for detecting overfitting and data leakage. Use it to validate the performance of your model and detect any potential sources of data leakage.\n",
    "4. Regularize your model: Regularization can help prevent overfitting and reduce the model’s reliance on specific features or subsets of the data.\n",
    "5. Avoid target leakage: Target leakage is when information about the target variable is inadvertently included in the input features. This can happen when we use features that are correlated with the target variable or when we perform preprocessing steps that depend on the target variable. To avoid target leakage, we should carefully select features that are independent of the target variable and perform preprocessing steps that do not use any information from the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8e96f-e588-41cc-bc0b-49a51cada1dd",
   "metadata": {},
   "source": [
    "# Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
    "\n",
    "A confusion matrix is a tabular summary of the number of correct and incorrect predictions made by a classifier. It is a matrix used to determine the performance of the classification models for a given set of test data. It  is a matrix that summarizes the performance of a machine learning model on a set of test data. It is often used to measure the performance of classification models, which aim to predict a categorical label for each input instance. The matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data. \n",
    "\n",
    "A confusion matrix can help us to evaluate how well our model can distinguish between different classes, and identify which classes are most often confused with each other. It can also help us to calculate various metrics, such as accuracy, precision, recall, and F1-score, that quantify the performance of your model.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d1ae17-8b39-49d4-904b-d8fab2540ccb",
   "metadata": {},
   "source": [
    "# Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
    "\n",
    "Difference between precision and recall are:\n",
    "\n",
    "Recall = True Positive/(True Positive + False Negative)\n",
    "\n",
    "Precision = True Positive/(True Positive + False Positive)  \n",
    "\n",
    "1. - It helps us to measure the ability to classify positive samples in the model.\n",
    "    - It helps us to measure how many positive samples were correctly classified by the ML model.\n",
    "\n",
    "2. - While calculating the Precision of a model, we should consider both Positive as well as Negative samples that are classified.\n",
    "    - While calculating the Recall of a model, we only need all positive samples while all negative samples will be neglected.\n",
    "\n",
    "3. - The precision of a machine learning model is dependent on both the negative and positive samples.\n",
    "    - Recall of a machine learning model is dependent on positive samples and independent of negative samples.\n",
    "\n",
    "4. - In Precision, we should consider all positive samples that are classified as positive either correctly or incorrectly.\n",
    "    - The recall cares about correctly classifying all positive samples. It does not consider if any negative sample is classified as positive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1c76b-6fe3-41de-acf6-2d8ec30290e4",
   "metadata": {},
   "source": [
    "# Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
    "\n",
    "A confusion matrix shows how many true positives, false positives, true negatives, and false negatives are produced by the model on a given test set. The rows of the matrix represent the predicted classes and the columns represent the actual classes. \n",
    "\n",
    "Types of Errors:\n",
    "- False positives (FP): These are the instances where the model predicts positive, but the actual label is negative. These are also known as Type I errors. \n",
    "\n",
    "- False negatives (FN): These are the instances where the model predicts negative, but the actual label is positive. These are also known as Type II errors. \n",
    "\n",
    "- True positives (TP): These are the instances where the model predicts positive, and the actual label is also positive. These indicate that the model is correctly identifying the positive class, and has a high recall.\n",
    "\n",
    "- True negatives (TN): These are the instances where the model predicts negative, and the actual label is also negative. These indicate that the model is correctly rejecting the negative class, and has a high precision.\n",
    "\n",
    "By analyzing the types of errors, we can determine the strengths and weaknesses of our model. For example, if our model has a high number of false positives, it is predicting positive instances when they are actually negative, indicating that the model has a low precision. On the other hand, if our model has a high number of false negatives, it is missing positive instances, indicating that the model has a low recall. By understanding the types of errors, we can adjust our model and improve its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02a8671-5c85-4d9b-8caa-0776ec719430",
   "metadata": {},
   "source": [
    "# Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n",
    "\n",
    "Some common metrics that can be derived from a confusion matrix are:\n",
    "- Accuracy: Accuracy simply measures how often the classifier makes the correct prediction. It is the ratio between the number of correct predictions and the total number of predictions. The accuracy metric is not suited for imbalanced classes. \n",
    "    Accuracy = (TP+TN)/(TP+TN+FN+FP)\n",
    "\n",
    "- Misclassification rate: It is also termed as Error rate, and it defines how often the model gives the wrong predictions. The value of error rate can be calculated as the number of incorrect predictions to all number of the predictions made by the classifier.\n",
    "    Error rate = (FN+FP)/(TP+TN+FN+FP)\n",
    "\n",
    "- Precision: It is a measure of correctness that is achieved in true prediction. In simple words, it tells us how many predictions are actually positive out of all the total positive predicted. Precision is a useful metric in cases where False Positive is a higher concern than False Negatives.\n",
    "    Precision = TP/(TP+FP)\n",
    "\n",
    "- Recall: It is a measure of actual observations which are predicted correctly, i.e. how many observations of positive class are actually predicted as positive. Recall is a useful metric in cases where False Negative trumps False Positive.\n",
    "    Recall = TP/(TP+FN)\n",
    "\n",
    "- F1-score: This is a harmonic mean of precision and recall. It tells how balanced or effective the model is in terms of both precision and recall.\n",
    "\n",
    "    F1-score = 2 * (Precision * Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194ac42f-ef5e-4e13-a981-d85f515b190f",
   "metadata": {},
   "source": [
    "# Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
    "\n",
    "The accuracy of a model is the ratio of total correct instances to the total instances. The values in its confusion matrix are the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data. The relation between the accuracy of the model and the values in its confusion matrix is given by the following formula:\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+TN+FN+FP)\n",
    "\n",
    "This means that the accuracy of the model depends on how many instances are correctly classified by the model as TP or TN, and how many instances are incorrectly classified by the model as FP or FN. The higher the TP and TN, and the lower the FP and FN, the higher the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012dcbee-18ff-45d4-ada8-e5439fc0fed8",
   "metadata": {},
   "source": [
    "# Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n",
    "\n",
    "A confusion matrix can help to identify potential biases or limitations in machine learning model by:\n",
    "\n",
    "- Showing how well the model performs for each class: \n",
    "    We can calculate metrics such as precision, recall, and F1-score for each class from the confusion matrix. These metrics can help to understand which classes are being predicted correctly and which classes are being confused or neglected by the model. For example, if we have a high precision but low recall for a class, it means that the model is good at identifying the instances of that class when they are present, but it misses many instances that should have been predicted as that class. This could indicate a bias or limitation in your model or data for that class.\n",
    "\n",
    "- Showing how balanced or imbalanced data is: \n",
    "    We can compare the number of actual instances for each class with the number of predicted instances for each class from the confusion matrix. This can help to see if the data has an even or uneven distribution of classes. For example, if we have a lot more instances of one class than another, it could lead to a skewed accuracy score that does not reflect the true performance of the model.\n",
    "\n",
    "- Showing how sensitive or specific your model is:\n",
    "    We can calculate metrics such as sensitivity and specificity from the confusion matrix. These metrics can help to understand how well the model can detect positive instances and avoid false alarms. For example, if we have a high sensitivity but low specificity for the model, it means that the model is good at identifying positive instances, but it also produces many false positives. This could indicate a limitation in the model or a trade-off between sensitivity and specificity that you need to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9a0a1-bb71-4c43-a995-0a27f0efb6be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
